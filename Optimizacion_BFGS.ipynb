{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAREA DE PROGRAMACIÓN No. 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computación evolutiva y optimización heurística - segundo semestre 2020\n",
    "## Universidad Nacional de Colombia - Sede Medellín\n",
    "### Profesora: Eddy ---\n",
    "\n",
    "### Alumnos\n",
    "- Edwin Fernando Villarraga Ossa\n",
    "-  Santiago Giraldo Escobar\n",
    "\n",
    "Noviembre de 2020\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OBJETIVO\n",
    "Utilizando una impelmentación del método de optimización Quasi-Newton de Broyden - Fletcher - Golford - Shanno (BFGS) en Python, se optimizarán 5 funciones para evaluar la capacidad de convergencia del método"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Descripción del método BFGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método BFGS pertenece al grupo de métodos de optimización denominados Quasi-Newton, estos métodos buscan eliminar algunas de las debilidades del método de Newton de Optimización, como son (Chandra, 2009):\n",
    "\n",
    "1. Se elimina la necesidad de calcular la matriz Hessiana\n",
    "2. Se ilimina la ncesidad de invertir la matriz Hessiana de segundas derivadas. Es posible que la matriz H no sea invertible.\n",
    "3. El método de Newton es costoso en terminos computacionales\n",
    "4. El método de Newton exige que la matriz H sea positiva definida.  La violación de este supuesto elimina la garantía de convergencia.\n",
    "\n",
    "Por lo tanto los métodos Quasi-Newton buscan la modificación de la matriz Hessiana por una aproximación de una matriz que debe cumplir algunas condiciones como que sea definida positiva pero que es más fácil de calcular que H.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método de Newton\n",
    "\n",
    "El método de Newton para encontrar raíces consiste en encontrar los puntos  $X \\epsilon R^n$  tales que $\\nabla f(X) = 0$\n",
    "\n",
    "Una aproximación de primer grado para encontrar estos puntos es:\n",
    "\n",
    "$X^{\\mathrm{(k+1)}} = X^{\\mathrm{(k)}}-(H_f(X^{\\mathrm{(k)}}))^{\\mathrm{-1}}\\nabla f(X^{\\mathrm{(k)}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Bibliografía"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Chandra Suresh, Jayadera y Mehra Aparna.  Numerical Optimization with Applications.  Narosa Publishing House.  2009.  Segunda Edicion.\n",
    "- Nocedal Jorge y Wright Stephen.  Numerical Optimization.  Springer.  2006.  Segunda Edicion.\n",
    "- Butenko Sergiy y Pardalos Panos.  Numerical Methods and Optimizations an Introduction.  Chapman & Hall.  2014.\n",
    "- Gramfort Alexandre.  (Quasi-)Newton methods.    online: http://www.lix.polytechnique.fr/bigdata/mathbigdata/wp-content/uploads/2014/09/notes_quasi_newton.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
