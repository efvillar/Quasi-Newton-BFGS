{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAREA DE PROGRAMACIÓN No. 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computación evolutiva y optimización heurística - segundo semestre 2020\n",
    "## Universidad Nacional de Colombia - Sede Medellín\n",
    "### Profesora: Eddy ---\n",
    "\n",
    "### Alumnos\n",
    "- Edwin Fernando Villarraga Ossa\n",
    "-  Santiago Giraldo Escobar\n",
    "\n",
    "Noviembre de 2020\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OBJETIVO\n",
    "Utilizando una impelmentación del método de optimización Quasi-Newton de Broyden - Fletcher - Golford - Shanno (BFGS) en Python, se optimizarán 5 funciones para evaluar la capacidad de convergencia del método"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> 1. Descripción del método BFGS </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método BFGS pertenece al grupo de métodos de optimización denominados Quasi-Newton, estos métodos buscan eliminar algunas de las debilidades del método de Newton de Optimización, como son (Chandra, 2009):\n",
    "\n",
    "1. Se elimina la necesidad de calcular la matriz Hessiana\n",
    "2. Se ilimina la ncesidad de invertir la matriz Hessiana de segundas derivadas. Es posible que la matriz H no sea invertible.\n",
    "3. El método de Newton es costoso en terminos computacionales\n",
    "4. El método de Newton exige que la matriz H sea positiva definida.  La violación de este supuesto elimina la garantía de convergencia.\n",
    "\n",
    "Por lo tanto los métodos Quasi-Newton buscan la modificación de la matriz Hessiana por una aproximación de una matriz que debe cumplir algunas condiciones como que sea definida positiva pero que es más fácil de calcular que H.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método de Newton\n",
    "\n",
    "El método de Newton para encontrar raíces consiste en encontrar los puntos  $X \\epsilon R^n$  tales que $\\nabla f(X) = 0$\n",
    "\n",
    "Una aproximación de segundo orden para encontrar estos puntos de forma iterativa es:\n",
    "\n",
    "$X^{\\mathrm{(k+1)}} = X^{\\mathrm{(k)}}-(H_f(X^{\\mathrm{(k)}}))^{\\mathrm{-1}}\\nabla f(X^{\\mathrm{(k)}})$\n",
    "\n",
    "Un ejemplo de la obtención de esta formula de recurrencia se puede encontrar en la página 341 de Chandra.\n",
    "\n",
    "\n",
    "### Métodos Quasi-Newton BFGS\n",
    "\n",
    "El método de BFGS es similar al método de lineserach de Newthon pero se utiliza una aproximación de la matriz Hessiana en el punto k denominada $B_k$ y en lugar de calcular una nueva matriz $B_k$ en cada iteración, se le realiza una actualziación.\n",
    "\n",
    "$B_{k+1} = B_k + \"algo\"$\n",
    "\n",
    "En el proceso iterativo es comun inicial con una matriz $B_0 = I_n$ para luaego continuar con las iteraciones.  \n",
    "\n",
    "La actualización de $B_k$ deber ser tal que se mantenga la relación:  $x_{k+1}-x_k=b_{k+1}(g_{k+1}-g_k)$ y que se mantenga definita positiva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen diferentes métodos de actualizar a $b_k$ y se denominan actualizaciones de rango 1 y 2 (Gramfort).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Fórmula de Broyden\n",
    "fórmula de actualización de rango1\n",
    "\n",
    "$B_{k+1} = B_k + \\LARGE \\frac{(s_k-B_ky_k)(s_k-B_ky_k)^T}{(s_k-B_ky_k)^Ty_k}$\n",
    "\n",
    "donde:\n",
    "\n",
    "\n",
    "$y_k = g_{k+1}-g_k = \\nabla f(x_{k+1}) - \\nabla f(x_k))$\n",
    "\n",
    "$s_k = x_{k+1}-x_k$ \n",
    "\n",
    "$g_k$ es el gradiente de la función en el punto k\n",
    "\n",
    "$y_k$ converge en menos de n iteración hacia la inversa de la Hessiana de f.  (ver prueba en Gramfort)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Fórmula de Davidon, Fletcher y Powell (DFP)\n",
    "\n",
    "\n",
    "Es una actualización de rango 2\n",
    "\n",
    "\n",
    "$B_{k+1} = B_k + \\LARGE \\frac{s_ks_k^T}{s_k^Ty_k} -  \\frac{B_ky_ky_k^TB_k}{y_k^TB_ky_k} $\n",
    "\n",
    "\n",
    "### Fórmula de Broyden-Fletcher-Goldfarb-Shanno\n",
    "Es una actualización de rango 2 que se deriva de la formula DFP intercambiando los roles de $s_k$ y $y_k$\n",
    "\n",
    "$H_{k+1} = H_k + \\LARGE \\frac{y_ky_k^T}{y_k^Ts_k} -  \\frac{H_ks_ks_k^TH_k}{s_k^TH_ky_k} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo codigo para BFGS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/pseudo.PNG\">\n",
    "\n",
    "fuente:  Alexandre Gramfort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el metodo DFD se actualiza la estimación de la inversa de la Hessiana. En el método BFGS se actualiza la esimación de la Hessiana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nocedal propone una implementación más eficiante que evita la necesidad de realziar inversión de matrices.  Se debe tener cuidado con la notación pues para Nocedal la matriz H es la inversa de la Hessiana, mientras que B es la estimación de la Hessiana (para Gramfort H representa la Hessiana y B la inversa de la Hessiana).\n",
    "\n",
    "Para Nocenal la formula recursiva para actualizar la inversa de la matriz Hessiana estimada es:\n",
    "\n",
    "$H_{k+1}=(I - \\rho s_k y_k^T)H_k(I - \\rho_ky_ks_k^T) + \\rho_ks_kS_k^T$\n",
    "\n",
    "donde:\n",
    "$\\rho_k = \\frac{1}{y_k^Ts_k}$\n",
    "\n",
    "A conitnuación se transcribe el pseudo código del libro de Nocedal\n",
    "\n",
    "<img src=\"./images/pseudo1.PNG\">\n",
    "\n",
    "Las condiciones de Wolfe son:\n",
    "\n",
    "<img src=\"./images/Wolfe.PNG\">\n",
    "\n",
    "que implica que el tamaño del paso debe ser tal que permita un decrecimiento en la función de manera proporcional tanto al tamaño del paso como a la dirección del gradiente, y la condición de curvatura con $C_2$ perteneciente a $(c_1,1)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> 2.  Implementación de BFGS </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar se importan las librarias necesarias para los calculos matriciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import misc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se contruye una función para las pruebas de implementación, en este caso se usará\n",
    "\n",
    "$f(x,y) = x^2 -xy + y^2 + 9x - 6y + 20$\n",
    "\n",
    "basado en la siguiente página:  https://sudonull.com/post/68834-BFGS-method-or-one-of-the-most-effective-optimization-methods-Python-implementation-example\n",
    "\n",
    "y se realizarán algunas iteraciones manualmente para ilustrar el método:\n",
    "\n",
    "Se toma como punto inicial:   $x_0 = (1,1)$\n",
    "\n",
    "Se define un nivel de precisión para la terminación del algoritmo,gradiente mínimo:  $\\epsilon = 0.001$\n",
    "\n",
    "Se calcula el gradiente de la funcion $\\nabla f = \\begin{bmatrix}2x - y +9 \\\\-x+2y-6\\end{bmatrix}$\n",
    "\n",
    "**Iteracion No. 1**\n",
    "se calcula el gradiente en el punto inicial\n",
    "\n",
    "$\\nabla f = \\begin{bmatrix}10\\\\-5\\end{bmatrix}$\n",
    "\n",
    "se evalua la condicion de terminación, con el gradiente mínimo:\n",
    "\n",
    "$|\\nabla f(x_0)| = \\sqrt{10^2 + (-5)^2} = 11.18 > 0.001$\n",
    "\n",
    "En este caso aún no se cumple la condición y se continúa con el algoritmo\n",
    "\n",
    "Se calcula entonces la dirección de busqueda:\n",
    "\n",
    "$p_0 = -H_0\\nabla f(x_0) = - \\begin{bmatrix}1 & 0\\\\0 & 1\\end{bmatrix} \\begin{bmatrix}10\\\\-5\\end{bmatrix} = \\begin{bmatrix}-10\\\\5\\end{bmatrix}$\n",
    "\n",
    "en este caso se tomo como $H_0$ a la matriz identidad\n",
    "\n",
    "se busca entonces el valor de $a_0$ para la actualización de $x_k$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$min(f(x_0 + \\alpha p_0))$\n",
    "\n",
    "$ x_0 + α_0 * p_0 = \\begin{bmatrix}1\\\\1\\end{bmatrix} + α_0 \\begin{bmatrix}-10\\\\5\\end{bmatrix}= \\begin{bmatrix}1 - 10α_0\\\\ 1 + 5α_0\\end{bmatrix} $\n",
    "\n",
    "se evalúa la función con el parametro $\\alpha_0$\n",
    "\n",
    "$ f (α_0) = (1 - 10α_0) ^ 2 - (1 - 10α_0) (1 + 5α_0) + (1 + 5α_0) ^ 2 + 9 (1 - 10α_0) - 6 (1 + 5α_0) + 20 $\n",
    "\n",
    "se simplifica, se deriva e iguala a cero para optener el tamaño optimo del paso\n",
    "\n",
    "$\\frac{\\partial f}{\\partial x} = 350 \\alpha_0 - 125 = 0$   por lo tanto $\\alpha_0 = 0.357$\n",
    "\n",
    "se calcula entonces el siguiente punto:\n",
    "\n",
    "$x_1 = x_0 + \\alpha_0 p_0 = \\begin{bmatrix}-2.571\\\\ 2.786\\end{bmatrix}$\n",
    "\n",
    "ahora se calcula $s_0$ que se requiere para la actualización de la estimación de la Hessiana\n",
    "\n",
    "$s_0 = x_1 - x_0 = \\begin{bmatrix}-2.571\\\\ 2.786\\end{bmatrix} - \\begin{bmatrix}1\\\\ 1\\end{bmatrix} = \\begin{bmatrix}-3.571\\\\ 1.786\\end{bmatrix}$\n",
    "\n",
    "se calcula entonces del valor de $y_0$\n",
    "\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Bibliografía"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Chandra Suresh, Jayadera y Mehra Aparna.  Numerical Optimization with Applications.  Narosa Publishing House.  2009.  Segunda Edicion.\n",
    "- Nocedal Jorge y Wright Stephen.  Numerical Optimization.  Springer.  2006.  Segunda Edicion.\n",
    "- Butenko Sergiy y Pardalos Panos.  Numerical Methods and Optimizations an Introduction.  Chapman & Hall.  2014.\n",
    "- Gramfort Alexandre.  (Quasi-)Newton methods.    online: http://www.lix.polytechnique.fr/bigdata/mathbigdata/wp-content/uploads/2014/09/notes_quasi_newton.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
