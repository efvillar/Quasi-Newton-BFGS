{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAREA DE PROGRAMACIÓN No. 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computación evolutiva y optimización heurística - segundo semestre 2020\n",
    "## Universidad Nacional de Colombia - Sede Medellín\n",
    "### Profesora: Eddy ---\n",
    "\n",
    "### Alumnos\n",
    "- Edwin Fernando Villarraga Ossa\n",
    "-  Santiago Giraldo Escobar\n",
    "\n",
    "Noviembre de 2020\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OBJETIVO\n",
    "Utilizando una impelmentación del método de optimización Quasi-Newton de Broyden - Fletcher - Goldfard - Shanno (BFGS) en Python, se optimizarán 5 funciones para evaluar la capacidad de convergencia del método"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> 1. Descripción del método BFGS </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método BFGS pertenece al grupo de métodos de optimización denominados Quasi-Newton, estos métodos buscan eliminar algunas de las debilidades del método de Newton de Optimización, como son (Chandra, 2009):\n",
    "\n",
    "1. Se elimina la necesidad de calcular la matriz Hessiana\n",
    "2. Se elimina la necesidad de invertir la matriz Hessiana de segundas derivadas. Es posible que la matriz H no sea invertible.\n",
    "3. El método de Newton es costoso en terminos computacionales\n",
    "4. El método de Newton exige que la matriz H sea positiva definida.  La violación de este supuesto elimina la garantía de convergencia.\n",
    "\n",
    "Por lo tanto los métodos Quasi-Newton buscan la modificación de la matriz Hessiana por una aproximación de una matriz que debe cumplir algunas condiciones como que sea definida positiva pero que es más fácil de calcular que H.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método de Newton\n",
    "\n",
    "El método de Newton para encontrar raíces consiste en encontrar los puntos  $X \\epsilon R^n$  tales que $\\nabla f(X) = 0$\n",
    "\n",
    "Una aproximación de segundo orden para encontrar estos puntos de forma iterativa es:\n",
    "\n",
    "$X^{\\mathrm{(k+1)}} = X^{\\mathrm{(k)}}-(H_f(X^{\\mathrm{(k)}}))^{\\mathrm{-1}}\\nabla f(X^{\\mathrm{(k)}})$\n",
    "\n",
    "Un ejemplo de la obtención de esta formula de recurrencia se puede encontrar en la página 341 de Chandra.\n",
    "\n",
    "\n",
    "### Métodos \"Quasi-Newton\"\n",
    "\n",
    "El método de BFGS es similar al método de Newthon pero se utiliza una aproximación de la matriz Hessiana en el punto k denominada $B_k$ y en lugar de calcular una nueva matriz $B_k$ en cada iteración, se le realiza una actualización.\n",
    "\n",
    "$B_{k+1} = B_k + \"algo\"$\n",
    "\n",
    "En el proceso iterativo es comun inicial con una matriz $B_0 = I_n$ para luego continuar con las iteraciones.  \n",
    "\n",
    "La actualización de $B_k$ deber ser tal que se mantenga la relación:  $x_{k+1}-x_k=b_{k+1}(g_{k+1}-g_k)$ y que se mantenga definita positiva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen diferentes métodos de actualizar a $b_k$ y se denominan actualizaciones de rango 1 y 2 (Gramfort).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Fórmula de Broyden\n",
    "fórmula de actualización de rango1\n",
    "\n",
    "$B_{k+1} = B_k + \\LARGE \\frac{(s_k-B_ky_k)(s_k-B_ky_k)^T}{(s_k-B_ky_k)^Ty_k}$\n",
    "\n",
    "donde:\n",
    "\n",
    "\n",
    "$y_k = g_{k+1}-g_k = \\nabla f(x_{k+1}) - \\nabla f(x_k))$\n",
    "\n",
    "$s_k = x_{k+1}-x_k$ \n",
    "\n",
    "$g_k$ es el gradiente de la función en el punto k\n",
    "\n",
    "$y_k$ converge en menos de n iteración hacia la inversa de la Hessiana de f.  (ver prueba en Gramfort)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Fórmula de Davidon, Fletcher y Powell (DFP)\n",
    "\n",
    "\n",
    "Es una actualización de rango 2\n",
    "\n",
    "\n",
    "$B_{k+1} = B_k + \\LARGE \\frac{s_ks_k^T}{s_k^Ty_k} -  \\frac{B_ky_ky_k^TB_k}{y_k^TB_ky_k} $\n",
    "\n",
    "\n",
    "### Fórmula de Broyden-Fletcher-Goldfarb-Shanno\n",
    "Es una actualización de rango 2 que se deriva de la formula DFP intercambiando los roles de $s_k$ y $y_k$\n",
    "\n",
    "$H_{k+1} = H_k + \\LARGE \\frac{y_ky_k^T}{y_k^Ts_k} -  \\frac{H_ks_ks_k^TH_k}{s_k^TH_ky_k} $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo codigo para BFGS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/pseudo.PNG\">\n",
    "\n",
    "fuente:  Alexandre Gramfort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el metodo DFP se actualiza la estimación de la inversa de la Hessiana. En el método BFGS se actualiza la esimación de la Hessiana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nocedal propone una implementación más eficiante que evita la necesidad de realizar inversión de matrices.  Se debe tener cuidado con la notación pues para Nocedal la matriz H es la inversa de la Hessiana, mientras que B es la estimación de la Hessiana (para Gramfort H representa la Hessiana y B la inversa de la Hessiana).\n",
    "\n",
    "Para Nocenal la formula recursiva para actualizar la inversa de la matriz Hessiana estimada es:\n",
    "\n",
    "$H_{k+1}=(I - \\rho_k s_k y_k^T)H_k(I - \\rho_ky_ks_k^T) + \\rho_ks_kS_k^T$\n",
    "\n",
    "donde:\n",
    "$\\rho_k = \\frac{1}{y_k^Ts_k}$\n",
    "\n",
    "A conitnuación se transcribe el pseudo código del libro de Nocedal\n",
    "\n",
    "<img src=\"./images/pseudo1.PNG\">\n",
    "\n",
    "Las condiciones de Wolfe son:\n",
    "\n",
    "<img src=\"./images/Wolfe.PNG\">\n",
    "\n",
    "que implica que el tamaño del paso debe ser tal que permita un decrecimiento en la función de manera proporcional tanto al tamaño del paso como a la dirección del gradiente, y la condición de curvatura con $C_2$ perteneciente a $(c_1,1)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style='background:yellow'> 2.  Implementación de BFGS</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar se importan las librarias necesarias para los calculos matriciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as ln\n",
    "import scipy as sp\n",
    "import scipy.optimize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se contruye una función para las pruebas de implementación, en este caso se usará\n",
    "\n",
    "$f(x,y) = x^2 -xy + y^2 + 9x - 6y + 20$\n",
    "\n",
    "basado en la siguiente página:  https://sudonull.com/post/68834-BFGS-method-or-one-of-the-most-effective-optimization-methods-Python-implementation-example\n",
    "\n",
    "y se realizarán algunas iteraciones manualmente para ilustrar el método:\n",
    "\n",
    "Se toma como punto inicial:   $x_0 = (1,1)$\n",
    "\n",
    "Se define un nivel de precisión para la terminación del algoritmo,gradiente mínimo:  $\\epsilon = 0.001$\n",
    "\n",
    "Se calcula el gradiente de la funcion $\\nabla f = \\begin{bmatrix}2x - y +9 \\\\-x+2y-6\\end{bmatrix}$\n",
    "\n",
    "-------\n",
    "\n",
    "\n",
    "**Iteracion No. 0**\n",
    "se calcula el gradiente en el punto inicial\n",
    "\n",
    "$\\nabla f = \\begin{bmatrix}10\\\\-5\\end{bmatrix}$\n",
    "\n",
    "se evalua la condicion de terminación, con el gradiente mínimo:\n",
    "\n",
    "$|\\nabla f(x_0)| = \\sqrt{10^2 + (-5)^2} = 11.18 > 0.001$\n",
    "\n",
    "En este caso aún no se cumple la condición y se continúa con el algoritmo\n",
    "\n",
    "Se calcula entonces la dirección de busqueda:\n",
    "\n",
    "$p_0 = -H_0\\nabla f(x_0) = - \\begin{bmatrix}1 & 0\\\\0 & 1\\end{bmatrix} \\begin{bmatrix}10\\\\-5\\end{bmatrix} = \\begin{bmatrix}-10\\\\5\\end{bmatrix}$\n",
    "\n",
    "en este caso se tomo como $H_0$ a la matriz identidad\n",
    "\n",
    "se busca entonces el valor de $a_0$ para la actualización de $x_k$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$min(f(x_0 + \\alpha p_0))$\n",
    "\n",
    "$ x_0 + α_0 * p_0 = \\begin{bmatrix}1\\\\1\\end{bmatrix} + α_0 \\begin{bmatrix}-10\\\\5\\end{bmatrix}= \\begin{bmatrix}1 - 10α_0\\\\ 1 + 5α_0\\end{bmatrix} $\n",
    "\n",
    "se evalúa la función con el parametro $\\alpha_0$\n",
    "\n",
    "$ f (α_0) = (1 - 10α_0) ^ 2 - (1 - 10α_0) (1 + 5α_0) + (1 + 5α_0) ^ 2 + 9 (1 - 10α_0) - 6 (1 + 5α_0) + 20 $\n",
    "\n",
    "se simplifica, se deriva e iguala a cero para optener el tamaño optimo del paso\n",
    "\n",
    "$\\frac{\\partial f}{\\partial x} = 350 \\alpha_0 - 125 = 0$   por lo tanto $\\alpha_0 = 0.357$\n",
    "\n",
    "se calcula entonces el siguiente punto:\n",
    "\n",
    "$x_1 = x_0 + \\alpha_0 p_0 = \\begin{bmatrix}-2.571\\\\ 2.786\\end{bmatrix}$\n",
    "\n",
    "ahora se calcula $s_0$ que se requiere para la actualización de la estimación de la Hessiana\n",
    "\n",
    "$s_0 = x_1 - x_0 = \\begin{bmatrix}-2.571\\\\ 2.786\\end{bmatrix} - \\begin{bmatrix}1\\\\ 1\\end{bmatrix} = \\begin{bmatrix}-3.571\\\\ 1.786\\end{bmatrix}$\n",
    "\n",
    "se calcula entonces del valor de $y_0$\n",
    "\n",
    "$y_0 = \\nabla f(x_1) - \\nabla f(x_0) = \\begin{bmatrix}1.071\\\\ 2.143\\end{bmatrix} - \\begin{bmatrix}10\\\\ -5\\end{bmatrix} = \\begin{bmatrix}-8.929\\\\ 7.143\\end{bmatrix}$\n",
    "\n",
    "para finalizar la iteración 0 se calcula entonces $H_1 = \\begin{bmatrix}0.694 & 0.367\\\\ 0.367 & 0.709\\end{bmatrix}$\n",
    "\n",
    "------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se define la función objetivo para las pruebas de implementación\n",
    "\n",
    "def f(x):\n",
    "    return x[0]**2 - x[0]*x[1] + x[1]**2 + 9*x[0] - 6*x[1] + 20\n",
    "  \n",
    "    \n",
    "# Derivative\n",
    "def f1(x):\n",
    "    return np.array([2 * x[0] - x[1] + 9, -x[0] + 2*x[1] - 6])\n",
    "\n",
    "\n",
    "def bfgs_method(f, fprime, x0, maxiter=None, epsi=10e-3):\n",
    "    \"\"\"\n",
    "    Función para minimizar la función f usando el BFGS\n",
    "    Parametros\n",
    "    ----------\n",
    "    func : f(x)- Función que se quiere minimizar\n",
    "    fprime : fprime(x) - gradiente de la función\n",
    "    x0 : ndarray - Punto inicial de la búsqueda\n",
    "    maxiter: int - Número máximo de iteraciones\n",
    "    epsi: double - Criterio de convergencia de la norma del gradiente\n",
    "    \"\"\"\n",
    "    if maxiter is None:\n",
    "        #Se asigna un límite al numero de iteraciones propocional al numero de variables\n",
    "        maxiter = len(x0) * 200\n",
    "    # valores iniciales para la iteración 0\n",
    "    #gfk es el gradiente de la función evaluado en x0, es un array de dos dimensiones, una por cada variable\n",
    "    k = 0\n",
    "    gfk = fprime(x0)\n",
    "    N = len(x0)\n",
    "    # Se define la matriz identidad que es la estimación inicial de la inversa de la Hessiana = Hk\n",
    "    I = np.eye(N, dtype=int)\n",
    "    Hk = I\n",
    "    xk = x0\n",
    "    #si la norma del gradiente en Xk es superior a epsilon, continua iterando y si no se ha superado el numero maximo de iteraciones\n",
    "    while ln.norm(gfk) > epsi and k < maxiter:\n",
    "        # pk - se calcula dirección de la búsqueda\n",
    "        pk = -np.dot(Hk, gfk)\n",
    "        # Se calcula la constante ak para la búsqueda lineal y que cumpla las condiciones de Wolfe\n",
    "        #se usa el método line_search de la clase optimize de la libreria scipy con el alias sp\n",
    "        # https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.line_search.html\n",
    "        line_search = sp.optimize.line_search(f, f1, xk, pk)\n",
    "        alpha_k = line_search[0]\n",
    "        # actualización de Xk a Xk+1\n",
    "        xkp1 = xk + alpha_k * pk\n",
    "        #se calcula sk necesario para la actualización de la Hessiana estimada\n",
    "        sk = xkp1 - xk\n",
    "        # se actualiza xk para la proxima iteración\n",
    "        xk = xkp1\n",
    "        #se calcula el gradiente en el nuevo punto xk\n",
    "        gfkp1 = fprime(xkp1)\n",
    "        # se calcula yk necesario para la actualización de la Hessiana estimada\n",
    "        yk = gfkp1 - gfk\n",
    "        # se actualiza gfk para la proxima iteración\n",
    "        gfk = gfkp1\n",
    "        k += 1\n",
    "        ro = 1.0 / (np.dot(yk, sk))\n",
    "        A1 = I - ro * sk[:, np.newaxis] * yk[np.newaxis, :]\n",
    "        A2 = I - ro * yk[:, np.newaxis] * sk[np.newaxis, :]\n",
    "        Hk = np.dot(A1, np.dot(Hk, A2)) + (ro * sk[:, np.newaxis] *\n",
    "                                                 sk[np.newaxis, :])\n",
    "    return (xk, k)\n",
    "result, k = bfgs_method(f, f1, np.array([1, 1]))\n",
    "print('Result of BFGS method:')\n",
    "print('Final Result (best point): %s' % (result))\n",
    "print('Iteration Count: %s' % (k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Bibliografía"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Chandra Suresh, Jayadera y Mehra Aparna.  Numerical Optimization with Applications.  Narosa Publishing House.  2009.  Segunda Edicion.\n",
    "- Nocedal Jorge y Wright Stephen.  Numerical Optimization.  Springer.  2006.  Segunda Edicion.\n",
    "- Butenko Sergiy y Pardalos Panos.  Numerical Methods and Optimizations an Introduction.  Chapman & Hall.  2014.\n",
    "- Gramfort Alexandre.  (Quasi-)Newton methods.    online: http://www.lix.polytechnique.fr/bigdata/mathbigdata/wp-content/uploads/2014/09/notes_quasi_newton.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
